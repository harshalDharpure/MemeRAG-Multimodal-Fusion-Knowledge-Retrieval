{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/RAG-Multimodal-Persuasive-Meme-Detection-/blob/main/Task_2__Persuaisve_Intensity_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vt6l1Q8EHYrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install chromadb tqdm fireworks-ai python-dotenv pandas\n",
        "!pip install sentence-transformers\n",
        "!pip install pytorch_lightning\n",
        "!pip install multilingual-clip\n",
        "!pip install langchain unstructured[all-docs] pydantic lxml"
      ],
      "metadata": {
        "id": "aRQh9dzTyXIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone -b v1.0 https://github.com/camenduru/LLaVA\n",
        "%cd /content/LLaVA\n",
        "\n",
        "!pip install -q transformers==4.36.2\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "tbtouDliyUqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
      ],
      "metadata": {
        "id": "qCwvwz-FyG8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# clip_model, compose = clip.load('ViT-L/14', device = device)\n",
        "# text_model = text_model.cpu()\n",
        "# def process(idx_val,arr):\n",
        "#   if idx_val=='0':\n",
        "#     arr.append(0)\n",
        "#   else:\n",
        "#     arr.append(1)"
      ],
      "metadata": {
        "id": "vVz4FPeNyG4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "import fireworks.client\n",
        "import dotenv\n",
        "import chromadb\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "T4wMzoV8yGzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Task2-Persuasive_Memes.csv')"
      ],
      "metadata": {
        "id": "U7n6LobUyGui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multilingual_clip import pt_multilingual_clip\n",
        "import transformers"
      ],
      "metadata": {
        "id": "CB56eAeqyGqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to check how multilingual clip works\n",
        "texts = [\n",
        "    'Three blind horses listening to Mozart.',\n",
        "    'Älgen är skogens konung!',\n",
        "    'Wie leben Eisbären in der Antarktis?',\n",
        "    'Вы знали, что все белые медведи левши?'\n",
        "]\n",
        "model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n",
        "# Load Model & Tokenizer\n",
        "model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "embeddings = model.forward(texts, tokenizer)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "Eopg6CXVyGmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = data['text'][10]\n",
        "sample"
      ],
      "metadata": {
        "id": "Jh1-aH0lyGY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "image = preprocess(Image.open('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/aloknath0.png')).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([sample]).to(device)"
      ],
      "metadata": {
        "id": "rcnGOA95x72P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# clip_model, compose = clip.load('RN50x4', device = device)\n",
        "clip_model, compose = clip.load(\"ViT-B/32\", device = device)\n",
        "text_inputs = (clip.tokenize(data.text.values[321],truncate=True)).to(device)\n",
        "print(text_inputs)"
      ],
      "metadata": {
        "id": "2wuuKCTux7xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "mt3IAeMpx7sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Name.values[1:10]"
      ],
      "metadata": {
        "id": "g3r0KfP2x7o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "caUztcj2TCCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fireworks.client   #HivLz5AE9LwlCJJOJD2kOKGFQ3LezDkQDI15zBAn3ysYXpGN\n",
        "\n",
        "import os\n",
        "fireworks.client.api_key = os.getenv(\"")\n",
        "fireworks.client.api_key = \"\"\n",
        "\n",
        "\n",
        "def get_completion(prompt, model=None, max_tokens=50):\n",
        "    fw_model_dir = \"accounts/fireworks/models/\"\n",
        "    if model is None:\n",
        "        model = fw_model_dir + \"llama-v2-7b\"\n",
        "    else:\n",
        "        model = fw_model_dir + model\n",
        "    completion = fireworks.client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        max_tokens=200,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return completion.choices[0].text\n",
        "\n",
        "\n",
        "def get_prompt_text(text_re):\n",
        "\n",
        "\n",
        "  prompt1 = f\"Retrieve relevant information about the meme with the text transcription: {text_re}  Explain the sentiment with context and any additional insights associated with this meme\"\n",
        "  return prompt1\n",
        "\n",
        "prompt1 = \"Retrieve relevant information about the meme with the text transcription: Explain the sentiment with context and any additional insights associated with this meme\"\n",
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "get_completion(prompt1, model=mistral_llm)\n",
        "\n",
        "\n",
        "\n",
        "#get_completion(prompt1+ data['Text_Transcription'][10], model=mistral_llm)"
      ],
      "metadata": {
        "id": "3fGwp5YbTEEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data):\n",
        "  #data = pd.read_csv(dataset_path)\n",
        "  text = list(data['text'])\n",
        "  img_path = list(data['Name'])\n",
        "  name = list(data['Name'])\n",
        "  persuasive_inten = list(data['persuasive_inten'])\n",
        "  # persuasive_inten = list(map(lambda x: x - 1 , persuasive_inten))\n",
        "  label = list(data['Persuasive'])\n",
        "  t3_1 =list(data['None'])\n",
        "  t3_2 = list(data['Negatively persuasive'])\n",
        "  t3_3 = list(data['Slightly Negatively persuasive'])\n",
        "  t3_4 = list(data['Neutral'])\n",
        "  t3_5 = list(data['Positively persuasive'])\n",
        "  t3_6 = list(data['Slightly Positively persuasive'])\n",
        "\n",
        "  text_features,image_features,rag_features,Name,l,t1,t2,t3,t4,t5,t6,persi = [],[],[],[],[],[],[],[],[],[],[],[]\n",
        "\n",
        "  for txt,img,L,n,a,b,c,d,e,f,v in tqdm(zip(text,img_path,label,name,t3_1,t3_2,t3_3,t3_4,t3_5,t3_6,persuasive_inten)):\n",
        "    try:\n",
        "      img = Image.open('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/'+img)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "    txt_rag = get_completion(get_prompt_text(txt)+txt, model=mistral_llm)\n",
        "    txt2 = txt_rag\n",
        "    img = torch.stack([compose(img).to(device)])\n",
        "    l.append(L)\n",
        "    Name.append(n)\n",
        "    t1.append(a)\n",
        "    t2.append(b)\n",
        "    t3.append(c)\n",
        "    t4.append(d)\n",
        "    t5.append(e)\n",
        "    t6.append(f)\n",
        "    persi.append(v)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      temp_rag=model.forward(txt2, tokenizer).detach().cpu().numpy()\n",
        "      rag_features.append(temp_rag)\n",
        "      temp_txt=model.forward(txt, tokenizer).detach().cpu().numpy()\n",
        "      text_features.append(temp_txt)\n",
        "      temp_img = clip_model.encode_image(img).detach().cpu().numpy()\n",
        "      image_features.append(temp_img)\n",
        "      del temp_txt\n",
        "      del temp_img\n",
        "      torch.cuda.empty_cache()\n",
        "    del img\n",
        "    torch.cuda.empty_cache()\n",
        "  return text_features,rag_features,image_features,l,Name,t1,t2,t3,t4,t5,t6,persi\n",
        "\n",
        "\n",
        "#Pre-Processing:\n",
        "#Converts the opened image (img) to a PyTorch tensor and stacks it into a batch\n",
        "#Uses CLIP to encode text into text_features & image to image_features\n",
        "#CLIP Uses Zero-Shot Learning_"
      ],
      "metadata": {
        "id": "YCtI2ujfg4M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features,rag_features,image_features,l,Name,t1,t2,t3,t4,t5,t6,persi = get_data(data.head(5))"
      ],
      "metadata": {
        "id": "bf08XCOPlPiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = []\n",
        "for names in tqdm(list(data['Name'])):\n",
        "  #change the path according to your drive\n",
        "  if not os.path.exists('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/'+names):\n",
        "    outliers.append(names)\n",
        "\n",
        "# data = data[~data['Name'].isin(outliers)]"
      ],
      "metadata": {
        "id": "iYKKDn6Fxz1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HatefulDataset(Dataset):\n",
        "\n",
        "  def __init__(self,data):\n",
        "    self.t_f,self.r_f,self.i_f,self.label,self.name, self.t3_1, self.t3_2, \\\n",
        "    self.t3_3, self.t3_4, self.t3_5, self.t3_6,self.persuasive_inten = get_data(data)\n",
        "\n",
        "    self.t_f = np.squeeze(np.asarray(self.t_f),axis=1)\n",
        "    self.r_f = np.squeeze(np.asarray(self.r_f),axis=1)\n",
        "    self.i_f = np.squeeze(np.asarray(self.i_f),axis=1)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    #print(idx)\n",
        "    name=self.name[idx]\n",
        "    label = self.label[idx]\n",
        "    persuasive_inten = self.persuasive_inten[idx]\n",
        "    t3_1 = self.t3_1[idx]\n",
        "    t3_2 = self.t3_2[idx]\n",
        "    t3_3 = self.t3_1[idx]\n",
        "    t3_4 = self.t3_4[idx]\n",
        "    t3_5 = self.t3_5[idx]\n",
        "    t3_6 = self.t3_6[idx]\n",
        "    T = self.t_f[idx,:]\n",
        "    R = self.r_f[idx,:]\n",
        "    I = self.i_f[idx,:]\n",
        "\n",
        "    sample = {'label':label,'processed_txt':T,'processed_rag':R,'processed_img':I,'name':name,'persuasive_inten':persuasive_inten,'None':t3_1,\n",
        "              'Negatively persuasive':t3_2,'Slightly Negatively persuasive': t3_3,'Neutral':t3_4,\n",
        "              'Positively persuasive':t3_5,'Slightly Positively persuasive':t3_6}\n",
        "    return sample\n",
        "\n",
        "#Dataset Class for easily maintaing pipeline\n",
        "\n",
        "#Init -> Pre-Process from get_data and convert to numpy array\n",
        "#Len -> Total Number of Sample\n",
        "#Getitem -> Sample at given idx in a List\n",
        "\n",
        "#Text & Image features -> RAG  Output goes through CLIP\n",
        "# We get otuput from multimodal MFB\n",
        "# apply attention mechanism  -> concatenate & apply softmax"
      ],
      "metadata": {
        "id": "T4tm-VGgjH6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dataset = HatefulDataset(data.head(500))"
      ],
      "metadata": {
        "id": "0gKa5wlyxEjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(sample_dataset,'/content/Task2_500_samples.pt')\n",
        "sample_dataset_new= torch.load(\"/content/Task2_500_samples.pt\")\n",
        "test= torch.load(\"/content/Task2_500_samples.pt\")\n",
        "len(sample_dataset_new)"
      ],
      "metadata": {
        "id": "qTy0z1k-xEfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MFB(nn.Module):\n",
        "    def __init__(self,img_feat_size, ques_feat_size, is_first, MFB_K, MFB_O, DROPOUT_R):\n",
        "        super(MFB, self).__init__()\n",
        "        #self.__C = __C\n",
        "        self.MFB_K = MFB_K\n",
        "        self.MFB_O = MFB_O\n",
        "        self.DROPOUT_R = DROPOUT_R\n",
        "        self.is_first = is_first\n",
        "        self.proj_i = nn.Linear(img_feat_size, MFB_K * MFB_O)\n",
        "        self.proj_q = nn.Linear(ques_feat_size, MFB_K * MFB_O)\n",
        "        self.dropout = nn.Dropout(DROPOUT_R)\n",
        "        self.pool = nn.AvgPool1d(MFB_K, stride = MFB_K)\n",
        "\n",
        "    def forward(self, img_feat, ques_feat, exp_in=1):\n",
        "        batch_size = img_feat.shape[0]\n",
        "        img_feat = self.proj_i(img_feat)                # (N, C, K*O)\n",
        "        ques_feat = self.proj_q(ques_feat)              # (N, 1, K*O)\n",
        "\n",
        "        exp_out = img_feat * ques_feat             # (N, C, K*O)\n",
        "        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)     # (N, C, K*O)\n",
        "        z = self.pool(exp_out) * self.MFB_K         # (N, C, O)\n",
        "        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
        "        z = F.normalize(z.view(batch_size, -1))         # (N, C*O)\n",
        "        z = z.view(batch_size, -1, self.MFB_O)      # (N, C, O)\n",
        "        return z\n",
        "\n",
        "\n",
        "#MFB -> Multimodal Factorized Bilinear Pooling\n",
        "#used to model complex interactions between features like image and text\n",
        "#MFB_K -> Number Of factors, MFB_O -> Output size,\n",
        "#Init initializes linear projection layers for image and question features , dropout layer and average pooling layer\n",
        "\n",
        "#Forward:\n",
        "\n",
        "#exp_in = input expansion factor (default - 1)\n",
        "#Linear projection of image and question features to factorized bilinear form\n",
        "#Element-wise multiplication of image and question features\n",
        "#APply Dropout\n",
        "#Average pooling along the factorized dimension (MFB_K) to reduce the size of the output tensor\n",
        "#Element-wise operations to compute the final output (z) using square root and normalization using Relu.\n",
        "#The final output represents the fused representation of image and question features.\n"
      ],
      "metadata": {
        "id": "gieUvt-gxEbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[~data['Name'].isin(outliers)]\n",
        "len(sample_dataset_new)"
      ],
      "metadata": {
        "id": "Qny4lgeExEW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "t_p,v_p = torch.utils.data.random_split(sample_dataset_new,[450,50])\n",
        "# torch.manual_seed(123)\n",
        "t_p,te_p = torch.utils.data.random_split(t_p,[340,110])"
      ],
      "metadata": {
        "id": "0uGdCCO_xESz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_p[1][\"processed_img\"].shape\n",
        "t_p[1]['processed_txt'].shape\n",
        "t_p[1]['processed_rag'].shape"
      ],
      "metadata": {
        "id": "I4b4FMJ7xEHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(512,768,True,256,64,0.1)\n",
        "    self.fin_y_shape = torch.nn.Linear(768,512)\n",
        "    self.fin_old = torch.nn.Linear(128,2)\n",
        "    self.fin = torch.nn.Linear(16 * 768, 64)\n",
        "    self.fin_inten = torch.nn.Linear(128,6)\n",
        "    self.fin_e1 = torch.nn.Linear(128,2)\n",
        "    self.fin_e2 = torch.nn.Linear(128,2)\n",
        "    self.fin_e3 = torch.nn.Linear(128,2)\n",
        "    self.fin_e4 = torch.nn.Linear(128,2)\n",
        "    self.fin_e5 = torch.nn.Linear(128,2)\n",
        "    self.fin_e6 = torch.nn.Linear(128,2)\n",
        "\n",
        "    self.validation_step_outputs = []\n",
        "    self.test_step_outputs = []\n",
        "\n",
        "  def forward(self, x,y,rag):\n",
        "      x_,y_,rag_ = x,y,rag\n",
        "      print(\"x.shape\", x.shape)\n",
        "      z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(x, axis=1))\n",
        "      z_rag = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(rag, axis=1))\n",
        "      z_newe = torch.cat((z, z_rag), dim=2)\n",
        "      z_new = torch.squeeze(z_newe, dim=1)\n",
        "      c_inten = self.fin_inten(z_new)\n",
        "      c_e1 = self.fin_e1(z_new)\n",
        "      c_e2 = self.fin_e2(z_new)\n",
        "      c_e3 = self.fin_e3(z_new)\n",
        "      c_e4 = self.fin_e4(z_new)\n",
        "      c_e5 = self.fin_e5(z_new)\n",
        "      c_e6 = self.fin_e6(z_new)\n",
        "      c = self.fin_old(z_new)\n",
        "      output = torch.log_softmax(c, dim=1)\n",
        "      c_inten = torch.log_softmax(c_inten, dim=1)\n",
        "      c_e1 = torch.log_softmax(c_e1, dim=1)\n",
        "      c_e2 = torch.log_softmax(c_e2, dim=1)\n",
        "      c_e3 = torch.log_softmax(c_e3, dim=1)\n",
        "      c_e4 = torch.log_softmax(c_e4, dim=1)\n",
        "      c_e5 = torch.log_softmax(c_e5, dim=1)\n",
        "      c_e6 = torch.log_softmax(c_e6, dim=1)\n",
        "\n",
        "      return output,c_inten,c_e1,c_e2,c_e3,c_e4,c_e5,c_e6\n",
        "\n",
        "\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      lab,txt,rag,img,name,intensity,e1,e2,e3,e4,e5,e6 = train_batch\n",
        "      lab = train_batch[lab]\n",
        "      #print(lab)\n",
        "      txt = train_batch[txt]\n",
        "      rag = train_batch[rag]\n",
        "      img = train_batch[img]\n",
        "      name= train_batch[name]\n",
        "      intensity = train_batch[intensity]\n",
        "      e1 = train_batch[e1]\n",
        "      e2 = train_batch[e2]\n",
        "      e3 = train_batch[e3]\n",
        "      e4 = train_batch[e4]\n",
        "      e5 = train_batch[e5]\n",
        "      e6 = train_batch[e6]\n",
        "\n",
        "      logit_offen,logit_inten_target,a,b,c,d,e,f= self.forward(txt,img,rag)\n",
        "\n",
        "      loss1 = self.cross_entropy_loss(logit_offen, lab)\n",
        "      loss4 = self.cross_entropy_loss(a, e1)\n",
        "      loss5 = self.cross_entropy_loss(b, e2)\n",
        "      loss6 = self.cross_entropy_loss(c, e3)\n",
        "      loss7 = self.cross_entropy_loss(d, e4)\n",
        "      loss8 = self.cross_entropy_loss(e, e5)\n",
        "      loss9 = self.cross_entropy_loss(f, e6)\n",
        "\n",
        "      loss17 = self.cross_entropy_loss(logit_inten_target, intensity)\n",
        "      loss = loss1 + loss4 + loss5 + loss6 + loss7 + loss8 +loss9 + loss17\n",
        "      self.log('train_loss', loss)\n",
        "      return loss\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      lab,txt,rag,img,name,intensity,e1,e2,e3,e4,e5,e6= val_batch\n",
        "      lab = val_batch[lab]\n",
        "      txt = val_batch[txt]\n",
        "      rag = val_batch[rag]\n",
        "      img = val_batch[img]\n",
        "      name = val_batch[name]\n",
        "      intensity = val_batch[intensity]\n",
        "      e1 = val_batch[e1]\n",
        "      e2 = val_batch[e2]\n",
        "      e3 = val_batch[e3]\n",
        "      e4 = val_batch[e4]\n",
        "      e5 = val_batch[e5]\n",
        "      e6 = val_batch[e6]\n",
        "\n",
        "      logits,inten,a,b,c,d,e,f = self.forward(txt,img,rag)\n",
        "      logits=logits.float()\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('val_acc', accuracy_score(lab,tmp))\n",
        "      self.log('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "      self.validation_step_outputs.append({'progress_bar': tqdm_dict,'val_f1 offensive': f1_score(lab,tmp,average='macro')})\n",
        "\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "      'val_f1 offensive': f1_score(lab,tmp,average='macro')\n",
        "      }\n",
        "\n",
        "  def on_validation_epoch_end(self):\n",
        "    outs = []\n",
        "    outs14=[]\n",
        "    for out in self.validation_step_outputs:\n",
        "       outs.append(out['progress_bar']['val_acc'])\n",
        "       outs14.append(out['val_f1 offensive'])\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_f1 offensive', sum(outs14)/len(outs14))\n",
        "    print(f'***val_acc_all_offn at epoch end {sum(outs)/len(outs)}****')\n",
        "    print(f'***val_f1 offensive at epoch end {sum(outs14)/len(outs14)}****')\n",
        "    self.validation_step_outputs.clear()\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      lab,txt,rag,img,name,intensity,e1,e2,e3,e4,e5,e6 = batch\n",
        "      lab = batch[lab]\n",
        "      txt = batch[txt]\n",
        "      rag = batch[rag]\n",
        "      img = batch[img]\n",
        "      name = batch[name]\n",
        "      intensity = batch[intensity]\n",
        "      e1 = batch[e1]\n",
        "      e2 = batch[e2]\n",
        "      e3 = batch[e3]\n",
        "      e4 = batch[e4]\n",
        "      e5 = batch[e5]\n",
        "      e6 = batch[e6]\n",
        "\n",
        "      logits,inten,a,b,c,d,e,f= self.forward(txt,img,rag)\n",
        "      logits = logits.float()\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(force=True),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('test_acc', accuracy_score(lab,tmp))\n",
        "      self.log('test_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('test_loss', loss)\n",
        "      tqdm_dict = {'test_acc': accuracy_score(lab,tmp)}\n",
        "      self.test_step_outputs.append({'progress_bar': tqdm_dict,'test_acc': accuracy_score(lab,tmp), 'test_f1_score': f1_score(lab,tmp,average='macro')})\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'test_acc': accuracy_score(lab,tmp),\n",
        "                'test_f1_score': f1_score(lab,tmp,average='macro')\n",
        "      }\n",
        "  def on_test_epoch_end(self):\n",
        "      outs = []\n",
        "      outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12= \\\n",
        "      [],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "      for out in self.test_step_outputs:\n",
        "        outs.append(out['test_acc'])\n",
        "        outs2.append(out['test_f1_score'])\n",
        "      self.log('test_acc', sum(outs)/len(outs))\n",
        "      self.log('test_f1_score', sum(outs2)/len(outs2))\n",
        "      self.test_step_outputs.clear()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=5e-3)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Main Model:\n",
        "Initialize\n",
        "Forward Pass\n",
        "Training Step\n",
        "Validation Step\n",
        "Testing Step\n",
        "\n",
        "Pp\n",
        "\"\"\"\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "    self.hm_train = t_p\n",
        "    self.hm_val = v_p\n",
        "    # self.hm_test = test\n",
        "    self.hm_test = te_p\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=64, drop_last=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=64, drop_last=True)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128, drop_last=True)\n",
        "\n",
        "data_module = HmDataModule()\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='mrinal/',\n",
        "     filename='epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "# train\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(123, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus=1\n",
        "#if torch.cuda.is_available():gpus=0\n",
        "trainer = pl.Trainer(deterministic=True,max_epochs=60,precision=16,callbacks=all_callbacks)\n",
        "trainer.fit(hm_model, data_module)"
      ],
      "metadata": {
        "id": "wkb7UAmjeAh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(dataset=te_p,batch_size=1478)\n",
        "ckpt_path = '/content/LLaVA/mrinal/epoch56-val_f1_all_offn0.77.ckpt' # put ckpt_path according to the path output in the previous cell\n",
        "trainer.test(dataloaders=test_dataloader,ckpt_path=ckpt_path)"
      ],
      "metadata": {
        "id": "rETvxMq2xOgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
