{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qvu1hdzGVXrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install chromadb tqdm fireworks-ai python-dotenv pandas\n",
        "!pip install sentence-transformers\n",
        "!pip install pytorch_lightning\n",
        "!pip install multilingual-clip\n",
        "!pip install langchain unstructured[all-docs] pydantic lxml"
      ],
      "metadata": {
        "id": "Gb_W9ccLVXj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone -b v1.0 https://github.com/camenduru/LLaVA\n",
        "%cd /content/LLaVA\n",
        "\n",
        "!pip install -q transformers==4.36.2\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "5T7ucD9SVXfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
      ],
      "metadata": {
        "id": "Cn1kPAwyVXcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# clip_model, compose = clip.load('ViT-L/14', device = device)\n",
        "# text_model = text_model.cpu()\n",
        "# def process(idx_val,arr):\n",
        "#   if idx_val=='0':\n",
        "#     arr.append(0)\n",
        "#   else:\n",
        "#     arr.append(1)"
      ],
      "metadata": {
        "id": "rACq5ozBVXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "import fireworks.client\n",
        "import dotenv\n",
        "import chromadb\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "SKMYcRCDVXUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Multitasking_Persuasive_Memes.csv')"
      ],
      "metadata": {
        "id": "gM0lP5yvXLWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "T_bZvRm6XMGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multilingual_clip import pt_multilingual_clip\n",
        "import transformers"
      ],
      "metadata": {
        "id": "Hvahmk7tXOUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to check how multilingual clip works\n",
        "texts = [\n",
        "    'Three blind horses listening to Mozart.',\n",
        "    'Älgen är skogens konung!',\n",
        "    'Wie leben Eisbären in der Antarktis?',\n",
        "    'Вы знали, что все белые медведи левши?'\n",
        "]\n",
        "model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n",
        "# Load Model & Tokenizer\n",
        "model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "embeddings = model.forward(texts, tokenizer)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "81mw2WSgXQ0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = data['text'][10]\n",
        "sample"
      ],
      "metadata": {
        "id": "D9GYbux3XTUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "image = preprocess(Image.open('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/aloknath0.png')).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([sample]).to(device)"
      ],
      "metadata": {
        "id": "JdsNcIS7XVtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "hZolKybgXYb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# clip_model, compose = clip.load('RN50x4', device = device)\n",
        "clip_model, compose = clip.load(\"ViT-B/32\", device = device)\n",
        "text_inputs = (clip.tokenize(data.text.values[321],truncate=True)).to(device)\n",
        "print(text_inputs)"
      ],
      "metadata": {
        "id": "2mqvFQcoXbIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "tDxroxwbXf4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Name.values[1:10]"
      ],
      "metadata": {
        "id": "BNU-WVWHXf0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "tfNvaTWcXiyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fireworks.client\n",
        "import os\n",
        "fireworks.client.api_key = os.getenv(\"zM8J4Kh5Ob3QGw4I3aN6xiX3GUee8Ola7wnMHat9cutf7XFx\")\n",
        "fireworks.client.api_key = \"zM8J4Kh5Ob3QGw4I3aN6xiX3GUee8Ola7wnMHat9cutf7XFx\"\n",
        "\n",
        "def get_completion(prompt, model=None, max_tokens=50):\n",
        "    fw_model_dir = \"accounts/fireworks/models/\"\n",
        "    if model is None:\n",
        "        model = fw_model_dir + \"llama-v2-7b\"\n",
        "    else:\n",
        "        model = fw_model_dir + model\n",
        "    completion = fireworks.client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        max_tokens=200,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return completion.choices[0].text\n",
        "\n",
        "def get_prompt_text(text_re):\n",
        "  prompt1 = f\"Retrieve relevant information about the meme with the text transcription: {text_re}  Explain the sentiment with context and any additional insights associated with this meme\"\n",
        "  return prompt1\n",
        "\n",
        "prompt1 = \"Retrieve relevant information about the meme with the text transcription: Explain the sentiment with context and any additional insights associated with this meme\"\n",
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "get_completion(prompt1, model=mistral_llm)\n",
        "\n",
        "#get_completion(prompt1+ data['Text_Transcription'][10], model=mistral_llm)"
      ],
      "metadata": {
        "id": "HztxHU0vXyCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "bQVTPSMWX3QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE7EuGIJVT1k"
      },
      "outputs": [],
      "source": [
        "def get_data(data):\n",
        "  #data = pd.read_csv(dataset_path)\n",
        "  text = list(data['text'])\n",
        "  img_path = list(data['Name'])\n",
        "  name = list(data['Name'])\n",
        "  persuasive_inten = list(data['persuasive_inten'])\n",
        "  #persuasive_inten = list(map(lambda x: x - 1 , persuasive_inten))\n",
        "  label = list(data['Persuasive'])\n",
        "  per_intem=list(data['persuasive_inten'])\n",
        "  t3_1 =list(data['Irony'])\n",
        "  t3_2 = list(data['personification'])\n",
        "  t3_3 = list(data['Alliteration'])\n",
        "  t3_4 = list(data['Analogies'])\n",
        "  t3_5 = list(data['Invective'])\n",
        "  t3_6 = list(data['Metaphor'])\n",
        "  t3_7 = list(data['puns_and_wordplays'])\n",
        "  t3_8 = list(data['Satire'])\n",
        "  t3_9 = list(data['Hyperboles'])\n",
        "  t1 = list(data['None'])\n",
        "  t2 = list(data['Negatively persuasive'])\n",
        "  t3 = list(data['Slightly Negatively persuasive'])\n",
        "  t4 = list(data['Neutral'])\n",
        "  t5 = list(data['Positively persuasive'])\n",
        "  t6 = list(data['Slightly Positively persuasive'])\n",
        "\n",
        "  text_features,image_features,rag_features,Name,l,ir,per,alli,ana,inv,meta,puaps,sat,hyp,persi,a_1,a_2,a_3,a_4,a_5,a_6 = [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "\n",
        "  for txt,img,L,n,a,b,c,d,e,f,g,h,i,v,j,k,q,m,n,o in tqdm(zip(text,img_path,label,name,t3_1,t3_2,t3_3,t3_4,t3_5,t3_6,t3_7,t3_8,t3_9,persuasive_inten,t1,t2,t3,t4,t5,t6)):\n",
        "    try:\n",
        "      img = Image.open('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/'+img)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "    txt_rag = get_completion(get_prompt_text(txt)+txt, model=mistral_llm)\n",
        "    txt2 = txt_rag\n",
        "    img = torch.stack([compose(img).to(device)])\n",
        "    l.append(L)\n",
        "    Name.append(n)\n",
        "    ir.append(a)\n",
        "    persi.append(v)\n",
        "    per.append(b)\n",
        "    alli.append(c)\n",
        "    ana.append(d)\n",
        "    inv.append(e)\n",
        "    meta.append(f)\n",
        "    puaps.append(g)\n",
        "    sat.append(h)\n",
        "    hyp.append(i)\n",
        "    a_1.append(j)\n",
        "    a_2.append(k)\n",
        "    a_3.append(q)\n",
        "    a_4.append(m)\n",
        "    a_5.append(n)\n",
        "    a_6.append(o)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      temp_rag=model.forward(txt2, tokenizer).detach().cpu().numpy()\n",
        "      rag_features.append(temp_rag)\n",
        "      temp_txt=model.forward(txt, tokenizer).detach().cpu().numpy()\n",
        "      text_features.append(temp_txt)\n",
        "      temp_img = clip_model.encode_image(img).detach().cpu().numpy()\n",
        "      image_features.append(temp_img)\n",
        "      del temp_txt\n",
        "      del temp_img\n",
        "      torch.cuda.empty_cache()\n",
        "    del img\n",
        "    torch.cuda.empty_cache()\n",
        "  return text_features,rag_features,image_features,l,Name,ir,per,alli,ana,inv,meta,puaps,sat,hyp,persi,a_1,a_2,a_3,a_4,a_5,a_6\n",
        "\n",
        "\n",
        "#Pre-Processing:\n",
        "#Converts the opened image (img) to a PyTorch tensor and stacks it into a batch\n",
        "#Uses CLIP to encode text into text_features & image to image_features\n",
        "#CLIP Uses Zero-Shot Learning_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trial\n",
        "text_features,rag_features,image_features,l,Name,ir,per,alli,ana,inv,meta,puaps,sat,hyp,persi,a_1,a_2,a_3,a_4,a_5,a_6 = get_data(data.head(5))"
      ],
      "metadata": {
        "id": "Mm-3F7MYYLO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = []\n",
        "for names in tqdm(list(data['Name'])):\n",
        "  #change the path according to your drive\n",
        "  if not os.path.exists('/content/drive/MyDrive/Gitanjali Mam/persuasive_meme/'+names):\n",
        "    outliers.append(names)\n",
        "\n",
        "# data = data[~data['Name'].isin(outliers)]"
      ],
      "metadata": {
        "id": "9D-WMCg4YPk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HatefulDataset(Dataset):\n",
        "\n",
        "  def __init__(self,data):\n",
        "\n",
        "    self.t_f,self.r_f,self.i_f,self.label,self.name,self.persuasive_inten,\\\n",
        "     self.t3_1, self.t3_2, self.t3_3, self.t3_4, self.t3_5,\\\n",
        "     self.t3_6, self.t3_7, self.t3_8, self.t3_9,self.t1, self.t2, self.t3,self.t4, self.t5, self.t6= get_data(data)\n",
        "    self.t_f = np.squeeze(np.asarray(self.t_f),axis=1)\n",
        "    self.r_f = np.squeeze(np.asarray(self.r_f),axis=1)\n",
        "    self.i_f = np.squeeze(np.asarray(self.i_f),axis=1)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.a)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    #print(idx)\n",
        "    T = self.t_f[idx,:]\n",
        "    R = self.r_f[idx,:]\n",
        "    I = self.i_f[idx,:]\n",
        "    name=self.name[idx]\n",
        "    label = self.label[idx]\n",
        "    persuasive_inten = self.persuasive_inten[idx]\n",
        "\n",
        "    t3_1 = self.t3_1[idx]\n",
        "    t3_2 = self.t3_2[idx]\n",
        "    t3_3 = self.t3_1[idx]\n",
        "    t3_4 = self.t3_4[idx]\n",
        "    t3_5 = self.t3_5[idx]\n",
        "    t3_6 = self.t3_6[idx]\n",
        "    t3_7 = self.t3_7[idx]\n",
        "    t3_8 = self.t3_8[idx]\n",
        "    t3_9 = self.t3_9[idx]\n",
        "    t1 = self.t1[idx]\n",
        "    t2 = self.t2[idx]\n",
        "    t3 = self.t3[idx]\n",
        "    t4 = self.t4[idx]\n",
        "    t5 = self.t5[idx]\n",
        "    t6 = self.t6[idx]\n",
        "\n",
        "    #name = self.name[idx]\n",
        "\n",
        "    sample = {'label':label,'processed_txt':T,'processed_rag':R,'processed_img':I,'name':name,'persuasive_inten':persuasive_inten,'irony':t3_1,'personification':t3_2,'Alliteration': t3_3,'Analogies':t3_4,\n",
        "              'Invective':t3_5,'Metaphor':t3_5,'puns_and_wordplays': t3_7,'Satire':t3_8,'Hyperboles':t3_9,'t1':t1, 't2':t2, 't3':t3, 't4':t4, 't5':t5, 't6':t6\n",
        "              }\n",
        "    return sample"
      ],
      "metadata": {
        "id": "5mp6uEOwYRgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dataset = HatefulDataset(data.head(500))"
      ],
      "metadata": {
        "id": "zxBt34DtaH1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(sample_dataset,'/content/persuasive_train50.pt')\n",
        "sample_dataset_new= torch.load(\"/content/new4000.pt\")\n",
        "test= torch.load(\"/content/new4000.pt\")\n",
        "len(sample_dataset_new)"
      ],
      "metadata": {
        "id": "JQwnkKRqaK9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MFB(nn.Module):\n",
        "    def __init__(self,img_feat_size, ques_feat_size, is_first, MFB_K, MFB_O, DROPOUT_R):\n",
        "        super(MFB, self).__init__()\n",
        "        #self.__C = __C\n",
        "        self.MFB_K = MFB_K\n",
        "        self.MFB_O = MFB_O\n",
        "        self.DROPOUT_R = DROPOUT_R\n",
        "        self.is_first = is_first\n",
        "        self.proj_i = nn.Linear(img_feat_size, MFB_K * MFB_O)\n",
        "        self.proj_q = nn.Linear(ques_feat_size, MFB_K * MFB_O)\n",
        "        self.dropout = nn.Dropout(DROPOUT_R)\n",
        "        self.pool = nn.AvgPool1d(MFB_K, stride = MFB_K)\n",
        "\n",
        "    def forward(self, img_feat, ques_feat, exp_in=1):\n",
        "        batch_size = img_feat.shape[0]\n",
        "        img_feat = self.proj_i(img_feat)                # (N, C, K*O)\n",
        "        ques_feat = self.proj_q(ques_feat)              # (N, 1, K*O)\n",
        "\n",
        "        exp_out = img_feat * ques_feat             # (N, C, K*O)\n",
        "        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)     # (N, C, K*O)\n",
        "        z = self.pool(exp_out) * self.MFB_K         # (N, C, O)\n",
        "        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
        "        z = F.normalize(z.view(batch_size, -1))         # (N, C*O)\n",
        "        z = z.view(batch_size, -1, self.MFB_O)      # (N, C, O)\n",
        "        return z\n",
        "\n",
        "\n",
        "#MFB -> Multimodal Factorized Bilinear Pooling\n",
        "#used to model complex interactions between features like image and text\n",
        "#MFB_K -> Number Of factors, MFB_O -> Output size,\n",
        "#Init initializes linear projection layers for image and question features , dropout layer and average pooling layer\n",
        "\n",
        "#Forward:\n",
        "\n",
        "#exp_in = input expansion factor (default - 1)\n",
        "#Linear projection of image and question features to factorized bilinear form\n",
        "#Element-wise multiplication of image and question features\n",
        "#APply Dropout\n",
        "#Average pooling along the factorized dimension (MFB_K) to reduce the size of the output tensor\n",
        "#Element-wise operations to compute the final output (z) using square root and normalization using Relu.\n",
        "#The final output represents the fused representation of image and question features.\n"
      ],
      "metadata": {
        "id": "74pz3z7Xaj2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[~data['Name'].isin(outliers)]\n",
        "len(sample_dataset_new)"
      ],
      "metadata": {
        "id": "azHKzCUHam11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "t_p,v_p = torch.utils.data.random_split(sample_dataset_new,[3500,500])\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "t_p,te_p = torch.utils.data.random_split(t_p,[3000,500])"
      ],
      "metadata": {
        "id": "Ht63aOEtapnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_p[1][\"processed_img\"].shape\n",
        "t_p[1]['processed_txt'].shape\n",
        "t_p[1]['processed_rag'].shape"
      ],
      "metadata": {
        "id": "k_DBcu4sardi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(512,768,True,192,64,0.1)\n",
        "    self.fin_y_shape = torch.nn.Linear(768,512)\n",
        "    self.fin_old = torch.nn.Linear(2048,2)\n",
        "    self.fin_e = nn.Linear(16 * 768, 64)  # Adjusted input size to match the reshaped output from MFB\n",
        "    self.fin_inten = torch.nn.Linear(2048,6)\n",
        "    self.fin_e1 = torch.nn.Linear(64,2)\n",
        "    self.fin_e2 = torch.nn.Linear(64,2)\n",
        "    self.fin_e3 = torch.nn.Linear(64,2)\n",
        "    self.fin_e4 = torch.nn.Linear(64,2)\n",
        "    self.fin_e5 = torch.nn.Linear(64,2)\n",
        "    self.fin_e6 = torch.nn.Linear(64,2)\n",
        "    self.fin_e7 = torch.nn.Linear(64,2)\n",
        "    self.fin_e8 = torch.nn.Linear(64,2)\n",
        "    self.fin_e9 = torch.nn.Linear(64,2)\n",
        "    self.validation_step_outputs = []\n",
        "    self.test_step_outputs = []\n",
        "\n",
        "  def forward(self,x,y,rag):\n",
        "      x_,y_,rag_ = x,y,rag\n",
        "      z= torch.cat((x, y, rag), dim=1)\n",
        "      z_new = torch.squeeze(z, dim=1)\n",
        "      c_inten = self.fin_inten(z_new)\n",
        "      c_e1 = self.fin_e1(z_new)\n",
        "      c_e2 = self.fin_e2(z_new)\n",
        "      c_e3 = self.fin_e3(z_new)\n",
        "      c_e4 = self.fin_e4(z_new)\n",
        "      c_e5 = self.fin_e5(z_new)\n",
        "      c_e6 = self.fin_e6(z_new)\n",
        "      c_e7 = self.fin_e7(z_new)\n",
        "      c_e8 = self.fin_e8(z_new)\n",
        "      c_e9 = self.fin_e9(z_new)\n",
        "      c = self.fin_old(z_new)\n",
        "\n",
        "      # probability distribution over labels\n",
        "      c = torch.log_softmax(c, dim=1)\n",
        "      c_inten = torch.log_softmax(c_inten, dim=1)\n",
        "      c_e1 = torch.log_softmax(c_e1, dim=1)\n",
        "      c_e2 = torch.log_softmax(c_e2, dim=1)\n",
        "      c_e3 = torch.log_softmax(c_e3, dim=1)\n",
        "      c_e4 = torch.log_softmax(c_e4, dim=1)\n",
        "      c_e5 = torch.log_softmax(c_e5, dim=1)\n",
        "      c_e6 = torch.log_softmax(c_e6, dim=1)\n",
        "      c_e7 = torch.log_softmax(c_e7, dim=1)\n",
        "      c_e8 = torch.log_softmax(c_e8, dim=1)\n",
        "      c_e9 = torch.log_softmax(c_e9, dim=1)\n",
        "\n",
        "      return z,c,c_a,c_v,c_e1,c_e2,c_e3,c_e4,c_e5,c_e6,c_e7,c_e8,c_e9,c_inten\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      lab,txt,rag,img,name,intensity,e1,e2,e3,e4,e5,e6,e7,e8,e9,t1,t2,t3,t4,t5,t6,t7 = train_batch\n",
        "      lab = train_batch[lab]\n",
        "      txt = train_batch[txt]\n",
        "      rag = train_batch[rag]\n",
        "      img = train_batch[img]\n",
        "      name= train_batch[name]\n",
        "      intensity = train_batch[intensity]\n",
        "      e1 = train_batch[e1]\n",
        "      e2 = train_batch[e2]\n",
        "      e3 = train_batch[e3]\n",
        "      e4 = train_batch[e4]\n",
        "      e5 = train_batch[e5]\n",
        "      e6 = train_batch[e6]\n",
        "      e7 = train_batch[e7]\n",
        "      e8 = train_batch[e8]\n",
        "      e9 = train_batch[e9]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(batch[t1],1),torch.unsqueeze(batch[t2],1),\\\n",
        "      torch.unsqueeze(batch[t3],1),torch.unsqueeze(batch[t4],1),torch.unsqueeze(batch[t5],1),\\\n",
        "      torch.unsqueeze(batch[t6],1),torch.unsqueeze(batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6),1)\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1)),1)\n",
        "\n",
        "      z,logit_offen, a,b,c,d,e,f,g,h,i,j,k,l,m,logitinten_target,logit_sarcasm,logit_emotion= self.forward(txt,img,rag)\n",
        "\n",
        "      loss1 = self.cross_entropy_loss(logit_offen, lab)\n",
        "      #loss2 = self.cross_entropy_loss(logit_arou, arou)\n",
        "      #loss3 = self.cross_entropy_loss(logit_val, val)\n",
        "      loss4 = self.cross_entropy_loss(a, e1)\n",
        "      loss5 = self.cross_entropy_loss(b, e2)\n",
        "      loss6 = self.cross_entropy_loss(c, e3)\n",
        "      loss7 = self.cross_entropy_loss(d, e4)\n",
        "      loss8 = self.cross_entropy_loss(e, e5)\n",
        "      loss9 = self.cross_entropy_loss(f, e6)\n",
        "\n",
        "      loss17 = self.cross_entropy_loss(inten, intensity)\n",
        "\n",
        "      loss18 = F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float())\n",
        "      loss_emo_mult = F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float())\n",
        "\n",
        "\n",
        "      #loss = loss1+loss_emo_mult+loss17\n",
        "      loss=loss1+loss_emo_mult\n",
        "      self.log('train_loss', loss)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      lab,txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,intensity,t1,t2,t3,t4,t5,t6 = val_batch\n",
        "\n",
        "      #print(val_batch)\n",
        "      lab = val_batch[lab]\n",
        "      txt = val_batch[txt]\n",
        "      img = val_batch[img]\n",
        "\n",
        "      e1 = val_batch[e1]\n",
        "      e2 = val_batch[e2]\n",
        "      e3 = val_batch[e3]\n",
        "      e4 = val_batch[e4]\n",
        "      e5 = val_batch[e5]\n",
        "      e6 = val_batch[e6]\n",
        "      e7 = val_batch[e7]\n",
        "      e8 = val_batch[e8]\n",
        "      e9 = val_batch[e9]\n",
        "\n",
        "\n",
        "      intensity = val_batch[intensity]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(val_batch[t1],1),torch.unsqueeze(val_batch[t2],1),\\\n",
        "      torch.unsqueeze(val_batch[t3],1),torch.unsqueeze(val_batch[t4],1),torch.unsqueeze(val_batch[t5],1),\\\n",
        "      torch.unsqueeze(val_batch[t6],1)\n",
        "      #print(t1.size())\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1)),1)\n",
        "\n",
        "      logits, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_emotion = self.forward(txt,img,rag)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('val_acc', f1_score(lab,tmp,average='macro'))\n",
        "      #self.log('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "      #print('Val acc {}'.format(accuracy_score(lab,tmp)))\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "              'val_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'val_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "              'val_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc intensity': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "      'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')\n",
        "      }\n",
        "\n",
        "  def validation_epoch_end(self, validation_step_outputs):\n",
        "    outs = []\n",
        "    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \\\n",
        "    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "    outs15 = []\n",
        "    outs18 = []\n",
        "    for out in validation_step_outputs:\n",
        "      outs.append(out['progress_bar']['val_acc'])\n",
        "      outs1.append(out['val_acc e1'])\n",
        "      outs2.append(out['val_acc e2'])\n",
        "      outs3.append(out['val_acc e3'])\n",
        "      outs4.append(out['val_acc e4'])\n",
        "      outs5.append(out['val_acc e5'])\n",
        "      outs6.append(out['val_acc e6'])\n",
        "      outs7.append(out['val_acc e7'])\n",
        "      outs8.append(out['val_acc e8'])\n",
        "      outs9.append(out['val_acc e9'])\n",
        "\n",
        "      outs14.append(out['val_acc intensity'])\n",
        "      outs15.append(out['val_loss_target'])\n",
        "      outs16.append(out['val_loss_emotion_multilabel'])\n",
        "      outs17.append(out['val_acc sarcasm'])\n",
        "      outs18.append(out['f1 sarcasm'])\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_loss_target', sum(outs15)/len(outs15))\n",
        "    self.log('val_acc_all e1', sum(outs1)/len(outs1))\n",
        "    self.log('val_acc_all e2', sum(outs2)/len(outs2))\n",
        "    self.log('val_acc_all e3', sum(outs3)/len(outs3))\n",
        "    self.log('val_acc_all e4', sum(outs4)/len(outs4))\n",
        "    self.log('val_acc_all e5', sum(outs5)/len(outs5))\n",
        "    self.log('val_acc_all e6', sum(outs6)/len(outs6))\n",
        "    self.log('val_acc_all e7', sum(outs7)/len(outs7))\n",
        "    self.log('val_acc_all e8', sum(outs8)/len(outs8))\n",
        "    self.log('val_acc_all e9', sum(outs9)/len(outs9))\n",
        "\n",
        "    self.log('val_acc_all inten', sum(outs14)/len(outs14))\n",
        "    self.log('val_loss_all emo', sum(outs16)/len(outs16))\n",
        "\n",
        "\n",
        "    print(f'***offensive f1 at epoch end {sum(outs)/len(outs)}****')\n",
        "    #print(f'***val acc inten at epoch end {sum(outs14)/len(outs14)}****')\n",
        "    print(f'***val loss emotion at epoch end {sum(outs16)/len(outs16)}****')\n",
        "    #print(f'***val acc sarcasm at epoch end {sum(outs17)/len(outs17)}****')\n",
        "    #print(f'***val f1 sarcasm at epoch end {sum(outs18)/len(outs18)}****')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      name,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,intensity,t1,t2,t3,t4,t5,t6 = batch\n",
        "      name = batch[name]\n",
        "      lab = batch[lab]\n",
        "      txt = batch[txt]\n",
        "      img = batch[img]\n",
        "      e1 = batch[e1]\n",
        "      e2 = batch[e2]\n",
        "      e3 = batch[e3]\n",
        "      e4 = batch[e4]\n",
        "      e5 = batch[e5]\n",
        "      e6 = batch[e6]\n",
        "      e7 = batch[e7]\n",
        "      e8 = batch[e8]\n",
        "      e9 = batch[e9]\n",
        "\n",
        "      intensity = batch[intensity]\n",
        "\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(batch[t1],1),torch.unsqueeze(batch[t2],1),\\\n",
        "      torch.unsqueeze(batch[t3],1),torch.unsqueeze(batch[t4],1),torch.unsqueeze(batch[t5],1),\\\n",
        "      torch.unsqueeze(batch[t6],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1)\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1)),1)\n",
        "\n",
        "      logits, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_emotion = self.forward(txt,img,rag)\n",
        "      #self.log('val_acc 1', accuracy_score(lab.detach().cpu().numpy(),np.argmax(logits.detach().cpu().numpy(),axis=-1)))\n",
        "      for n in name:\n",
        "        N.append(n)\n",
        "      append_gt(lab,o_t); append_gt(e1,e1_t); append_gt(e2,e2_t); append_gt(e3,e3_t); append_gt(e4,e4_t); append_gt(e5,e5_t);\\\n",
        "      append_gt(e6,e6_t); append_gt(e7,e7_t); append_gt(e8,e8_t); append_gt(e9,e9_t); append_gt(intensity,i_t);\n",
        "\n",
        "      append_p(logits,o_p); append_p(a,e1_p); append_p(b,e2_p); append_p(c,e3_p); append_p(d,e4_p); append_p(e,e5_p);\\\n",
        "      append_p(f,e6_p); append_p(g,e7_p); append_p(h,e8_p); append_p(i,e9_p); append_p(inten,i_p);\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('test_acc', accuracy_score(lab,tmp))\n",
        "      self.log('test f1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro'))\n",
        "      np.save('multitask_logit_emotion.npy',logit_emotion.detach().cpu().numpy())\n",
        "      np.save('multitask_logit_offensive.npy',lab)\n",
        "      np.save('multitask_logit_intensity.npy',inten.detach().cpu().numpy())\n",
        "      #self.log('test confusion matrix',confusion_matrix(lab,tmp))\n",
        "      #print(f'confusion matrix {confusion_matrix(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix intensity {confusion_matrix(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix offensive {confusion_matrix(lab,tmp)}')\n",
        "\n",
        "      #self.log('test_roc_auc',roc_auc_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('F1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('recall',recall_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('precision',precision_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      best_threshold = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5])\n",
        "      y_test = torch.nn.Sigmoid()(logit_emotion)\n",
        "      y_test = y_test.detach().cpu().numpy()\n",
        "      y_pred = np.array([[1 if y_test[i][j]>=best_threshold[j] else 0 for j in range(13)] for i in range(len(y_test))])\n",
        "      #print(y_pred)\n",
        "      total_correctly_predicted = len([i for i in range(len(y_test)) if (y_test[i]==y_pred[i]).sum() == 13])\n",
        "      self.log('test_loss', loss)\n",
        "      #print(total_correctly_predicted)\n",
        "      pred_e = y_test\n",
        "      return {'test_loss': loss,\n",
        "              'test_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'test_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "               'test_acc':f1_score(lab,tmp,average='macro'),\n",
        "              'test_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc inten': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "              'test_acc sarcasm': accuracy_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)),\n",
        "              'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')}\n",
        "  def test_epoch_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        outs = []\n",
        "        outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \\\n",
        "        [],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "        outs15 = []\n",
        "        outs16 = []\n",
        "        outs17 = []\n",
        "        outs18 = []\n",
        "        for out in outputs:\n",
        "          outs15.append(out['test_loss_target'])\n",
        "          outs.append(out['test_acc'])\n",
        "          outs1.append(out['test_acc e1'])\n",
        "          outs2.append(out['test_acc e2'])\n",
        "          outs3.append(out['test_acc e3'])\n",
        "          outs4.append(out['test_acc e4'])\n",
        "          outs5.append(out['test_acc e5'])\n",
        "          outs6.append(out['test_acc e6'])\n",
        "          outs7.append(out['test_acc e7'])\n",
        "          outs8.append(out['test_acc e8'])\n",
        "          outs9.append(out['test_acc e9'])\n",
        "\n",
        "          outs14.append(out['test_acc inten'])\n",
        "          outs16.append(out['test_acc sarcasm'])\n",
        "          outs17.append(out['test_loss_emotion_multilabel'])\n",
        "\n",
        "\n",
        "        #print(outs)\n",
        "        self.log('final test f1', sum(outs)/len(outs))\n",
        "        \"\"\"\n",
        "        self.log('test_acc_all e1', sum(outs1)/len(outs1))\n",
        "        self.log('test_acc_all e2', sum(outs2)/len(outs2))\n",
        "        self.log('test_acc_all e3', sum(outs3)/len(outs3))\n",
        "        self.log('test_acc_all e4', sum(outs4)/len(outs4))\n",
        "        self.log('test_acc_all e5', sum(outs5)/len(outs5))\n",
        "        self.log('test_acc_all e6', sum(outs6)/len(outs6))\n",
        "        self.log('test_acc_all e7', sum(outs7)/len(outs7))\n",
        "        self.log('test_acc_all e8', sum(outs8)/len(outs8))\n",
        "        self.log('test_acc_all e9', sum(outs9)/len(outs9))\n",
        "        self.log('test_acc_all e10', sum(outs10)/len(outs10))\n",
        "        self.log('test_acc_all e11', sum(outs11)/len(outs11))\n",
        "        self.log('test_acc_all e12', sum(outs12)/len(outs12))\n",
        "        self.log('test_acc_all e13', sum(outs13)/len(outs13))\n",
        "        self.log('test_acc_all inten', sum(outs14)/len(outs14))\n",
        "        self.log('test_loss_all target', sum(outs15)/len(outs15))\n",
        "        self.log('test_acc_all sarcasm', sum(outs16)/len(outs16))\n",
        "        \"\"\"\n",
        "        self.log('test_loss_all emo', sum(outs17)/len(outs17))\n",
        "        #self.log('test_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=5e-3)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "\n",
        "\n",
        "\n",
        "    self.hm_train = t_p\n",
        "    self.hm_val = v_p\n",
        "    self.hm_test = te_p\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128)\n",
        "\n",
        "data_module = HmDataModule()\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='noemo/ckpts/',\n",
        "     filename='our-ds-ckpt-epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "\"\"\"\n",
        "for i in range(1,14):\n",
        "  tmp_checkpoint_callback = ModelCheckpoint(\n",
        "      monitor='val_acc_all e{}'.format(i),\n",
        "      dirpath='noemo/ckpts/e{}'.format(i),\n",
        "      filename='our-ds-ckpt-best-emo-{}'.format(i),\n",
        "      auto_insert_metric_name=False,\n",
        "      save_top_k=1,\n",
        "      mode=\"max\",\n",
        "  )\n",
        "  all_callbacks.append(tmp_checkpoint_callback)\n",
        "\"\"\"\n",
        "# train\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(seed=123, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus,max_epochs=60,callbacks=all_callbacks)\n",
        "#trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=60,callbacks=all_callbacks)\n",
        "\n",
        "trainer.fit(hm_model, data_module)\n"
      ],
      "metadata": {
        "id": "TUmMcDVnjqii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(dataset=te_p,batch_size=20)\n",
        "ckpt_path = '/content/LLaVA/mrinal/epoch00-val_f1_all_offn0.00.ckpt' # put ckpt_path according to the path output in the previous cell\n",
        "trainer.test(dataloaders=test_dataloader,ckpt_path=ckpt_path)"
      ],
      "metadata": {
        "id": "sKbdwwTfjr4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}